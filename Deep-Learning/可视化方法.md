# 可视化方法

## 1.激活最大化方法

​	核心思想：寻找 **$\textcolor{red}{最大化特定层神经元激活值}$** 的输入模式

​	相当于寻找$X$，使得$AX$最大，$A$为激活函数

​	缺点：

-    优化过程含有噪声，导致输入难以解释

-    激活最大化只使用于连续的数据，难以解释离散的数据

  

## 2.基于梯度解释方法

​	核心思想：利用$\textcolor{red}{反向传播}$计算特定输出相对于输入的$\textcolor{red}{梯度}$来推导出特征的重要性

​	缺点：

- ​	解释结果存在清晰的噪声，无法判断噪声是否真实反应模型决策依据
- ​    梯度信息只能定位重要特征，无法判断每个特征对决策结果的贡献程度

### 反卷积（Deconvolution）

反卷积（Deconvolution）方法包括三个部分：反池化（Unpooling)，反激活(Rectification)，反卷积（Deconvolution）

反池化：

<img src="https://wth-markdown-image.oss-cn-beijing.aliyuncs.com/markdown_img/image-20221122110415346.png" alt="image-20221122110415346" style="zoom:50%;" />

反激活：

需要保证每层的特征图为正值，与激活过程一样采用ReLu函数

卷积过程：

将输入按行展开成列向量，如下图的浅蓝色列向量。根据卷积在实际中的对应相乘位置，将卷积块展开成对应矩阵。卷积块与输入进行几次运算，就展开成几行。每一行代表卷积块在每一次运算中对应输入的位置。

<img src="https://wth-markdown-image.oss-cn-beijing.aliyuncs.com/markdown_img/image-20221122103956188.png" alt="image-20221122103956188" style="zoom: 33%;" />

该过程可以描述为：
$$
AX=b
$$
反卷积过程（Deconvolution）：

该过程需要通过$b$求得$X$，实际上该过程是有无穷多组解的，因此反卷积操作也不能够恢复原来的值

<img src="https://wth-markdown-image.oss-cn-beijing.aliyuncs.com/markdown_img/image-20221122105655957.png" alt="image-20221122105655957" style="zoom: 33%;" />

该过程可以描述为：
$$
Y=A^Tb
$$
反卷积过程实际上就是用卷积核去对于输出进行一次padding下的卷积，如下图所示：

<img src="https://wth-markdown-image.oss-cn-beijing.aliyuncs.com/markdown_img/image-20221122110100534.png" alt="image-20221122110100534" style="zoom: 50%;" />

### 导向反向传播（Guided Back-propagation）

<img src="https://wth-markdown-image.oss-cn-beijing.aliyuncs.com/markdown_img/image-20221122110800080.png" alt="image-20221122110800080" style="zoom:67%;" />

反激活：

前向传播：用Re-Lu作为激活函数，传递输入大于0的部分

反向传播：输入大于0的部分的梯度被反向传播回去

反卷积方法：梯度大于0的梯度传回去

导向反向传播：输入和梯度均大于0的梯度传回去

## 3.基于类激活映射方法

### 1. CAM

<img src="https://wth-markdown-image.oss-cn-beijing.aliyuncs.com/markdown_img/image-20221122112337097.png" alt="image-20221122112337097" style="zoom: 45%;" />