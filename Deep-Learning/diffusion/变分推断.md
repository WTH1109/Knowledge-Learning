# 变分推断

## 背景：

从贝叶斯角度来看，优化问题可以视为一个积分问题

设$x$为数据先验，$\theta$为数据后验

由贝叶斯公式：
$$
p(\theta | x)=\frac{p(x|\theta)p(\theta)}{p(x)}
$$
在贝叶斯决策当中，假设我们已知$N$个样本$x$，现在要做决策求新的样本$\tilde{x}$的分布，即$p(\tilde{x}|x)$
$$
p(\tilde{x}|x)=\int_{\theta}p(\tilde{x},\theta|x)d\theta=\int_{\theta}p(\tilde{x}|\theta)p(\theta|x)d\theta=E_{\theta|x}[p(\tilde{x}|\theta)]
$$
变分推断（VI）是一种对于后验概率$p(\theta|x)$的确定性近似

## KL散度

KL散度用于衡量两个数据集的分布情况

具体定义为交叉熵与信息熵的差值

即
$$
KL(p||q)=H(p,q)-H(q)=-E_p[\log q]+E_p[\log p]=\sum p_i \log\frac{p_i}{q_i}
$$

## 变分推断

$X：$观测数据(observed data)

$Z：$隐变量+参数（latent variable + parameter)

$(X,Z):$完全数据（complete data）

我们的目标是找到一个$q(Z)$,使之近似于$p(Z|X)$
$$
\log p(X)=\log p(X,Z)-\log p(Z|X)=\log \frac{p(X,Z)}{q(Z)}-\log \frac{p(Z|X)}{q(Z)}
$$
两边求关于$q(z)$分布的期望
$$
LHS=\int_z p(X)q(Z)dZ=p(X)
$$

$$
RHS=\int_Z q(Z)\log\frac{p(X,Z)}{q(Z)}dz-\int_Z q(Z)\log \frac{p(Z|X)}{q(Z)}dz=ELBO+KL(q(Z)||p(Z|X))
$$


$$
=\mathcal{L}(q)+KL(q||p)
$$
$\mathcal{L}(q)$是对$q$的一个变分，$KL(q||p)\geq0$

因此，为了达到我们的目标，我们需要让$KL(q||p)$尽可能接近于0，即让$\mathcal{L}(q)$尽可能大

这样的话，原来的找到接近的分布问题就变成了一个优化问题，即
$$
\hat{q}(Z)=\arg \max_{q(Z)}\mathcal{L}(q)\Rightarrow \hat{q}(Z)=p(Z|X)
$$

$$
q(Z)=\prod_{i=1}^{M}q_i(Z_i)
$$

$$
\mathcal{L}(q)=\int_Z q(Z)\log\frac{p(X,Z)}{q(Z)}dz=\int_Z q(Z)\log p(X,Z)dz-\int_Z q(Z)\log q(Z)dz
$$

下面分别考虑$\mathcal{L}(q)$式中的两项

对第一项进行化简：
$$
\int_Z q(Z)\log p(X,Z)dz=\int_Z \prod_{i=1}^{M}q_i(Z_i)\log p(X,Z)dz_1dz_2...dz_n
$$

$$
=\int_Z q_j(Z_j)\prod_{i\neq j}^{M}q_i(Z_i)\log p(X,Z)dz_1dz_2...dz_n=\int_{Z_j} q_j(Z_j)\int_{Z_i i\neq j}\prod_{i\neq j}^{M}q_i(Z_i)\log p(X,Z)dz_1dz_2...dz_n dz_j
$$

$$
=\int_{Z_j} q_j(Z_j)E_{\prod_{i\neq j}^{M}q_i(Z_i)}[\log p(X,Z)] dz_j=\int_{Z_j} q_j(Z_j)\log\hat P(X,Z_j) dz_j
$$

对第二项进行化简
$$
\int_Z q(Z)\log q(Z)dz=\int_Z \prod_{i=1}^{M}q_i(Z_i)\log \prod_{i=1}^{M}q_i(Z_i)dz=\int_Z \prod_{i=1}^{M}q_i(Z_i)[\sum_{i=1}^{M}\log q_i(Z_i)]dz
$$

$$
\int_Z \prod_{i=1}^{M}q_i(Z_i)\log q_1(Z_1)dz=\int_Z q_1 \log q_1 dz_1 \int q_2(Z_2)dz_2...\int q_n(Z_n)dz_n=\int_Z q_1 \log q_1 dz_1
$$

$$
\int_Z \prod_{i=1}^{M}q_i(Z_i)[\sum_{i=1}^{M}\log q_i(Z_i)]dz=\sum_{i=1}^M  \int_Z q_i \log q_i dz=\int_{Z_j} q_j \log q_j dz_j +C
$$

因此
$$
\mathcal{L}(q)=\int_Z q(Z)\log p(X,Z)dz-\int_Z q(Z)\log q(Z)dz=\int_{Z_j} q_j(Z_j)\log\frac{\hat P(X,Z_j)}{ q_j(Z_j)} dz_j +C
$$

$$
=-KL(q_j||\hat P(X,Z_j))+C
$$

为了使$\mathcal{L}(q)$尽可能大，由此可以看出我们需要让$KL(q_j||\hat P(X,Z_j))$尽可能小

即
$$
q_j(Z_j)=\hat P(X,Z_j)
$$

## 回顾

基本假设
$$
q(Z)=\prod_{i=1}^{M}q_i(Z_i)
$$
即$Z$可以分为$M$份独立的部分

简要思路

1.因为要求后验
$$
p(Z|X)
$$
我们希望找到一个东西去逼近它，即
$$
q(Z)
$$
为了尽可能地逼近，我们需要$KL$散度尽可能小，即
$$
\hat{q}=\arg \min \{KL(q(Z)||p(Z|X))\}
$$
2.由于后验可以进行分解
$$
KL(q(Z)||p(Z|X))=\int q(Z) \log \frac{q(Z)}{p(Z|X)}dZ=\int q(Z) \log \frac{q(Z)p(X)}{p(Z,X)}dZ
$$

$$
=E[\log q(Z)]-E[\log p(Z,X)] + E[\log p(X)]=-ELBO + \log p(X)
$$

因此
$$
ELBO=E[\log p(Z,X)]-E[\log q(Z)]
$$

$$
ELBO=\log p(X) - KL(q(Z)||p(Z|X))
$$

$$
\log p(X) = KL(q(Z)||p(Z|X)) + ELBO\geq ELBO
$$

$ELBO:$ Evidence lower bound 证据下界

$p(X)$是evidence
