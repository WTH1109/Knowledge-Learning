# 决策树模型

### 节点的选择

**离散情况**

1）$ID3$：信息增益

信息增益：选择的节点X使得类Y的不确定性减少的程度

信息增益问题：不适合解决种类比较多的特征，比如ID特征。ID特征为$1\sim n$，但是熵必然为0，信息增益最大，但是按照ID分没有任何用处

2）$C4.5$:信息增益率
$$
\frac{G}{H(X)}
$$
$G$代表节点信息熵，$H(X)$代表自信息熵

3）$CART$：使用$GINI$系数当作衡量标准
$$
Gini(p)=\sum_{k=1}^{K}p_k(1-p_k)=1-\sum_k^Kp_k^2
$$
**连续情况**

将连续值分段，找到熵最小的分法

### 减枝

**预剪枝**

限制深度：只能用固定的特征数去决策

限制叶子节点个数

限制信息增益量

**后剪枝**

建立完树后再进行剪枝

<img src="https://wth-markdown-image.oss-cn-beijing.aliyuncs.com/markdown_img/image-20230410102727328.png" alt="image-20230410102727328" style="zoom: 80%;" />



$$
C_{\alpha}(T)=C(T)+\alpha\cdot|T_{leaf}|
$$
相当于把节点的权重加入到分裂的判别当中

$\alpha$越大，越难分裂，模型不容易过拟合



### 利用决策树进行决策

**分类**

叶节点的众数作为分类类别

**回归**

用类内方差代替信息熵进行节点的分裂

叶节点的平均数作为预测值



